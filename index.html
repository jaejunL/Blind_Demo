<!DOCTYPE HTML>
<!--
	Telephasic by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->

<html>
	<head>
		<title>SWS Demo</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="homepage is-preload">
		<div id="page-wrapper">
			<!-- Header -->
				<div id="header-wrapper">
						<section id="hero" class="container">
							<header>
								<h2 style="line-height :1.2;"><font size=7><strong>
									<!-- (The page will be built untill this week) -->
								Speaking Without Sound:<br />
								Multi-speaker Silent Speech Voicing<br />
								with Facial Inputs Only</strong></font></h2>
							</header>
							<p style="line-height :1.4;"><font size=5><em>Jaejun Lee, Yoori Oh, and Kyogu Lee</em><br />
							Music and Audio Research Group (MARG), Seoul National University</font></p>
							<p></p>
							<hr style="width:75%;color:white">
							<h2><font size=6>Abstract</font></h2>
							<div class="col-6 col-12-medium" style="max-width:2000px; margin-top: 20px; margin-bottom: 40px;">
								<p style="line-height :1.2;"><font size=5>
								In this paper, we introduce a novel framework for generating multi-speaker speech without relying on any audible inputs.
								Our approach leverages silent electromyography (EMG) signals to capture linguistic content, while facial images are used to align with the vocal identity of the target speaker.
								Notably, we present a pitch-disentangled content embedding that enhances the extraction of linguistic content from EMG signals.
								Extensive analysis demonstrates that our method can successfully generate multi-speaker speech without any audible inputs and confirms the effectiveness of the proposed pitch-disentanglement approach.</br></br></br>
								</font></p>
								<p><img align="center" src="media/figpng.png" style="  display: block ;
									margin-left: auto;
									margin-right: auto;
									width: 100%;">
									<p style="line-height :1;"><font size=3>
										Figure : Overview of the proposed multi-speaker EMG-to-Speech generation framework. In the (a) Inference phase, content embedding is estimated from EMG signals, while speaker embedding and global pitch information are derived from a facial image.
										During the (b) Training phase, for training the (i) face-based voice conversion network, a predefiend speaker-wise global pitch is used to estimate frame-wise F0 values. However, since the global pitch values for unseen target speakers are not available during the inference, a (iii) face-based global pitch estimation network is independently trained using only face images.
										Additionally, as the speech-based content encoder is not available during inference, the (ii)~EMG-based contents estimation network is trained.
										Each network is trained independently during the training phase.</font></p>		
								</p>
							</div>
							<p></p>
							<p></p>
							<!-- <ul class="actions"> -->
								<!-- <li><a href="#" class="button">Get this party started</a></li> -->
							<!-- </ul> -->
						</section>
				</div>

			<!-- Features 1 -->
				<div class="wrapper">
					<div class="container">
						<section id="one">
						<header class="major">
							<h2 align="left">Conversion samples</h2>
						</header>
						<header>
							<h3 style="line-height: 1.2"><font size="4">
							<strong>Note:</strong> All the source input here consists of <strong><em>silent EMG</em></strong> signals,
							meaning there is no audible sound when recorded.
							The audio in the source part is vocalized speech paired with <em>voiced EMG</em>,
							recorded independently but containing the same content as the silent EMG,
							meaning that the script for both EMG signals was identical.
							</font></h3>
							<!-- <h5 style="line-height: 1.4; color:black;">Utilize speech audio from the Source and a facial image from the Target.</br> -->
								<!-- (<b>HYFace</b>: The proposed method. <b>FVMVC</b>: The benchmark. Sheng <img src="https://latex.codecogs.com/png.latex?\dpi{100}\color{black} et \hspace{1mm} al"/>., 2023)</h5> -->
								<!-- <br> -->
						</header>
						<header>
							<h3 style="line-height: 2">1. Multi-speaker conversion samples</h3>
							<h5 style="line-height: 1.4; color:black;">Utilize the content information from the input <strong><em>silent EMG</em></strong> and a facial image from the target.</br>
								The conversion model used here is the proposed model with a pitch-flattening module.</br>
								We recommend playing the source speech first, then listening to the converted speech
								while looking at the target face image.<br>
								This is because the goal of this research is not to match the target voice exactly,
								but to align with the target face.<br>
								We suggest playing the target voice as the last step.<br>
						</header>						
						<h4 style="line-height: 3"><strong>Sample 1 (male)</strong></h4>
						<!-- <table stype='width:60%' style="text-align: center;"> -->
						<table stype='width:100%' style="text-align: center;">
							<tr>
								<td style="text-align: center; width:30%; height:30%;">Source EMG<br />
									<img src="media/emg.png" style="width:30%;height:30%" alt=""></img></br>
									<h5 style="line-height: 1."><em>"He read and re-read the paper, fearing the worst had happened to me."</em></h5>
									<audio controls="" style='display: inline-block; padding:2px; width: 240px; height: 50px'><source src="media/sample1/source_350_audio_clean.wav" type="audio/wav"></audio>
								</td>
								<td style="text-align: center; width:30%; height:30%"">Target Face<br />
									<img src="media/sample1/target_image.jpg" style="width:50%" alt=""></img></br>
									<audio controls="" style='display: inline-block; padding:2px; width: 240px; height: 50px;'><source src="media/sample1/target_audio.wav" type="audio/wav"></audio>
								</td>

								<td style="text-align: center; width:40%"" width="400px">Converted<br />
									<br /><br />
									<audio controls="" style='padding:2px; width: 280px; height: 50px'>
									<source src="media/sample1/convert_9_5-10_silent_350_emg_tiSiSM3pk5M_00002_0002.wav" type="audio/wav"></audio>
								</td>
							</tr>
						</table>

						<br /><br />
						<br /><br />

						</section>
					</div>
				</div>
		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>